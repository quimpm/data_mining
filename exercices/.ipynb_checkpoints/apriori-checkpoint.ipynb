{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bf668e6-df23-4431-bedd-f6771058f2fa",
   "metadata": {},
   "source": [
    "## Apriori Exercise \n",
    "\n",
    "Consider the following programming exercise. Given the information of the frequent singletons $(L_1)$ and frequent pairs $(L_2)$ we compute with our previous implementation of A-Priori for k=2, implement in spark functions to compute the  **confidence** and **interest** of all the binary rules we can build from the set $L_2$. As the dataset to test your code, **compute** a set of transactions from this data set:\n",
    "\n",
    "https://www.kaggle.com/datasets/heeraldedhia/groceries-dataset\n",
    "\n",
    "Or this smaller one (that it already contains one transaction per line) if you are not able to work with the previous one:\n",
    "\n",
    "https://www.kaggle.com/shazadudwadia/supermarket?select=GroceryStoreDataSet.csv\n",
    "\n",
    "\n",
    "Try these values for $\\theta$: 0.01, 0.1, 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c6d4e3-3b91-4e58-bd45-eb576dfdc8e6",
   "metadata": {},
   "source": [
    "Once you have computed the sets T, L1, $T_{L1}$ and L2, your program should follow these steps:\n",
    "\n",
    "1. Map, using mapPartitions, each frequent pair in the RDD with L2 to its list of binary association rules (two association rules per each different frequent pair). Use then the flattened version of this RDD.\n",
    "2. Map each association rule of the previous resulting RDD, to a triple with (rule,confidence,interest). Observe that you will need to use the information in L1 and the number of transactions to compute these values. You can use the version of L1 stored as a python list in the driver (so it can be passed inside functions passed to spark tasks).\n",
    "3. Finally, sort the association rules by their interest, and show back in the driver program the first 10 most interesting rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45ffe63-c254-4901-af2d-71dacf5210c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\\n\",\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "print ( spark_home )\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "sc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
