{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e42256",
   "metadata": {},
   "source": [
    "# Second Exercise: Cosine similarity for movie comparison\n",
    "\n",
    "In this exercise you have to implement in a python notebook using the spark framework:\n",
    "\n",
    "1. The distributed (map/reduce) algorithm of slide \"3.7\" (in notebook \"8-Item-to-Items-globalfiltering-recommenders-py3-sshow.ipynb\") for computing the cosine similarity of a set of products with negative and positive ratings, using as input information an RDD (or spark dataframe that is also distributed) with ratings with this format:\n",
    "\n",
    "     (userID,movieID,rating)\n",
    "\n",
    "2. The computation of the Cosine Similarity (with the previous algorithm) of all the pairs of movies from the different files you have with this exercise:\n",
    "  filtered50movies.csv filtered100movies.csv  filtered150movies.csv   filtered200movies.csv\n",
    "\n",
    "Each file contains ratings for a different set of movies, but the ones in a smaller file\n",
    "are always a subset of a file with bigger size. We provide files with different size\n",
    "in case you have some memory issues in your computer, so use the biggest file you are able to use, although during \"testing\" of your code you can of course use the smallest file, or even any smaller subset of the file filtered50movies.csv.\n",
    "\n",
    "3. Show on the screen the information for the \"top 10\" most similar pairs, but using the\n",
    "name of the movies you can find in the file movies.\n",
    "\n",
    "All the steps should be implemented always with map/reduce operations with spark RDDs/dataframes. Except the last step, when you have to find the name of the movies in the top-ten recommendations.\n",
    "\n",
    "Present your notebook with plenty of comments in all your functions.\n",
    "\n",
    "NOTE: The ratings for movies come from a dataset obtained from the smallest dataset from:\n",
    "https://grouplens.org/datasets/movielens/\n",
    "But the ratings have been re-scaled from the range [0,5] to the range [-3,2.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c6b94f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import psutil\n",
    "from itertools import combinations\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ef4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\\n\",\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "print ( spark_home )\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f148e6",
   "metadata": {},
   "source": [
    "First, i'm going to add some lines to charge the different files, I'm not gonna charge all of them because I want as much memory free as I can to perform the biggest computation possible with my PC specifications. **Select and run the cell of the dataaset that you want to execute the code with!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f63a3a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = sc.textFile(\"filtered50movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e5f74bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = sc.textFile(\"filtered100movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "442b6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = sc.textFile(\"filtered150movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "97716a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = sc.textFile(\"filtered200movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3347cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = sc.textFile(\"movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ced57284",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies.map(lambda x: x.split(',')).collect()\n",
    "moviesRDD = sc.parallelize(movies[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ff9a56f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '1', '1.5'],\n",
       " ['5', '1', '1.5'],\n",
       " ['7', '1', '2.0'],\n",
       " ['15', '1', '-0.5'],\n",
       " ['17', '1', '2.0']]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviesRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7298dea",
   "metadata": {},
   "source": [
    "Now let's create the pairs to obtain the startid DataStructure containing the pairs. \n",
    "$$  (u,p_1,r_1),(u,p_2,r_2) $$\n",
    "To do it we will group by user, create all possible combinations of pairs and flat the result in a single list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c2ec0947",
   "metadata": {},
   "outputs": [],
   "source": [
    "moviePairsByUserRDD = moviesRDD \\\n",
    ".groupBy(lambda x: x[0]).map(lambda x : list(combinations(list(x[1]),2))).flatMap(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "bb3a9984",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quimpm/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "  warnings.warn(\"Please install psutil to have better \" \"support with spilling\")\n",
      "/Users/quimpm/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "  warnings.warn(\"Please install psutil to have better \" \"support with spilling\")\n",
      "/Users/quimpm/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "  warnings.warn(\"Please install psutil to have better \" \"support with spilling\")\n",
      "/Users/quimpm/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "  warnings.warn(\"Please install psutil to have better \" \"support with spilling\")\n",
      "/Users/quimpm/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "  warnings.warn(\"Please install psutil to have better \" \"support with spilling\")\n",
      "/Users/quimpm/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "  warnings.warn(\"Please install psutil to have better \" \"support with spilling\")\n",
      "/Users/quimpm/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "  warnings.warn(\"Please install psutil to have better \" \"support with spilling\")\n",
      "/Users/quimpm/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "  warnings.warn(\"Please install psutil to have better \" \"support with spilling\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['44', '1', '0.5'], ['44', '3', '0.5']),\n",
       " (['44', '1', '0.5'], ['44', '6', '0.5']),\n",
       " (['44', '1', '0.5'], ['44', '260', '2.5']),\n",
       " (['44', '1', '0.5'], ['44', '648', '0.5']),\n",
       " (['44', '1', '0.5'], ['44', '661', '0.5'])]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviePairsByUserRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1ed066",
   "metadata": {},
   "source": [
    "Now we can apply the transformation:\n",
    "$$  (u,p_1,r_1),(u,p_2,r_2) \\rightarrow ((p_1,p_2),(r_1 r_2,r_1^2,r_2^2) ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9beab510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('1', '3'), (0.25, 0.25, 0.25)),\n",
       " (('1', '6'), (0.25, 0.25, 0.25)),\n",
       " (('1', '260'), (1.25, 0.25, 6.25)),\n",
       " (('1', '648'), (0.25, 0.25, 0.25)),\n",
       " (('1', '661'), (0.25, 0.25, 0.25))]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieRatingComputations = moviePairsByUserRDD \\\n",
    "    .map(lambda x: ((x[0][1], x[1][1]), (float(x[0][2])*float(x[1][2]),float(x[0][2])**2, float(x[1][2])**2) ))\n",
    "movieRatingComputations.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0f4d60",
   "metadata": {},
   "source": [
    "Now we can apply the reduction:\n",
    "$$ ((p_1,p_2),(pra_{1,2},ra_1^2,ra_2^2) ) + ((p_1,p_2),(prb_{1,2},rb_1^2,rb_2^2) ) \n",
    "   \\rightarrow \\\\  ((p_1,p_2),( pra_{1,2}+prb_{1,2}, ra_1^2+rb_1^2,\n",
    "   ra_2^2+rb_2^2) ) $$\n",
    "To do so it's used the reduceByKey operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "555c44f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('1', '736'), (98.0, 225.25, 132.5)),\n",
       " (('3', '260'), (30.5, 48.75, 134.75)),\n",
       " (('6', '260'), (194.25, 186.25, 296.0)),\n",
       " (('648', '1552'), (34.5, 90.0, 72.5)),\n",
       " (('733', '1552'), (31.5, 69.25, 55.5))]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieRatingComputationsReduced = movieRatingComputations.reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1], x[2]+y[2]))\n",
    "movieRatingComputationsReduced.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f2a93",
   "metadata": {},
   "source": [
    "Finally, we compute the cosine distance like:\n",
    "    \n",
    "$$ ((p_1,p_2),(\\sum_u r_1 r_2,\\sum_u r_1^2,\\sum_u r_2^2) ) \\rightarrow \n",
    "\\frac{\\sum_u r_1 r_2}{\\sqrt{\\sum_u r_1^2} \\sqrt{\\sum_u r_2^2}}  $$ \n",
    "\n",
    "This is going to be done with a simple map operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "adbf23d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('1', '736'), 0.5672646709965458),\n",
       " (('3', '260'), 0.37631206489056224),\n",
       " (('6', '260'), 0.8273076329051288),\n",
       " (('648', '1552'), 0.42709927780721924),\n",
       " (('733', '1552'), 0.5081058245382075)]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity = movieRatingComputationsReduced \\\n",
    "    .map(lambda x: (x[0], x[1][0]/(math.sqrt(x[1][1])*math.sqrt(x[1][2]))))\n",
    "cosine_similarity.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf345e",
   "metadata": {},
   "source": [
    "Now we are going to sort the results and show the top 10 more similar movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "990809ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('151', '441'), 1.0),\n",
       " (('362', '2616'), 0.9922778767136677),\n",
       " (('2944', '3034'), 0.9855258295520649),\n",
       " (('362', '2949'), 0.9832820049844603),\n",
       " (('362', '2478'), 0.9785497849867492),\n",
       " (('151', '2580'), 0.9782797401561579),\n",
       " (('1805', '3034'), 0.977897823397447),\n",
       " (('151', '2470'), 0.9734503756241593),\n",
       " (('2018', '2944'), 0.9689627902499088),\n",
       " (('362', '2580'), 0.9666666666666666)]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity.sortBy(lambda x: x[1], ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6400a3",
   "metadata": {},
   "source": [
    "**There we go! Our top 10!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
