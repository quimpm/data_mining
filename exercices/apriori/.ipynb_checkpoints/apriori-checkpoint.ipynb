{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bf668e6-df23-4431-bedd-f6771058f2fa",
   "metadata": {},
   "source": [
    "## Apriori Exercise \n",
    "\n",
    "Consider the following programming exercise. Given the information of the frequent singletons $(L_1)$ and frequent pairs $(L_2)$ we compute with our previous implementation of A-Priori for k=2, implement in spark functions to compute the  **confidence** and **interest** of all the binary rules we can build from the set $L_2$. As the dataset to test your code, **compute** a set of transactions from this data set:\n",
    "\n",
    "https://www.kaggle.com/datasets/heeraldedhia/groceries-dataset\n",
    "\n",
    "Or this smaller one (that it already contains one transaction per line) if you are not able to work with the previous one:\n",
    "\n",
    "https://www.kaggle.com/shazadudwadia/supermarket?select=GroceryStoreDataSet.csv\n",
    "\n",
    "\n",
    "Try these values for $\\theta$: 0.01, 0.1, 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c6d4e3-3b91-4e58-bd45-eb576dfdc8e6",
   "metadata": {},
   "source": [
    "Once you have computed the sets T, L1, $T_{L1}$ and L2, your program should follow these steps:\n",
    "\n",
    "1. Map, using mapPartitions, each frequent pair in the RDD with L2 to its list of binary association rules (two association rules per each different frequent pair). Use then the flattened version of this RDD.\n",
    "2. Map each association rule of the previous resulting RDD, to a triple with (rule,confidence,interest). Observe that you will need to use the information in L1 and the number of transactions to compute these values. You can use the version of L1 stored as a python list in the driver (so it can be passed inside functions passed to spark tasks).\n",
    "3. Finally, sort the association rules by their interest, and show back in the driver program the first 10 most interesting rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e42b2b-0917-4bb8-a066-60d6c31fbe85",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45ffe63-c254-4901-af2d-71dacf5210c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/03 21:09:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.133:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\\n\",\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "print ( spark_home )\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec9dcb7-9267-405e-9ae5-60dfd412cf0d",
   "metadata": {},
   "source": [
    "## Notebook code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a076fac0-c2b8-45b1-bd62-064057605cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the rdd with frequent singleton sets (L_1)\n",
    "def computeL1 ( rddT, numtrans, theta ):\n",
    "  rddtemp = rddT.flatMap( lambda t : [ (it,1) for it in t ] ).reduceByKey( lambda a,b : a+b  )\n",
    "  return rddtemp.filter( lambda x : (float(x[1])/numtrans) >= theta )\n",
    "\n",
    "# Map any transaction to its version without elements not in L1\n",
    "# L1 must be a python list, not a RDD\n",
    "def computeTfilteredByL1( seqOfT, L1 ):\n",
    "    for t in seqOfT:\n",
    "       yield [ it for it in t if (it in L1) ]\n",
    "    \n",
    "# For each t in seqofFilteredT (they come from T_{L_1}), compute pairs (a,b) from t that belong to C_2\n",
    "def generateC2( seqofFilteredT ):\n",
    "    for t in seqofFilteredT:\n",
    "      cpairslist = []\n",
    "      for (a,b) in [ (a,b) for i,a in enumerate(t[:-1]) for b in t[i+1:] ]:\n",
    "                cpairslist.append( ((a,b),1) if (a <= b) else ((b,a),1)  )         \n",
    "      yield cpairslist\n",
    "    \n",
    "def computeL2( rddC2T, numtrans, theta ):\n",
    "    pairsCountedrdd = rddC2T.reduceByKey( lambda v1,v2 : v1+v2 )\n",
    "    # Finally, filter out from the previous rdd those pairs with frequency below theta\n",
    "    return pairsCountedrdd.filter( lambda x : (float(x[1])/numtrans) >= theta )\n",
    "\n",
    "def calculate_two_iterations(groceries, theta):\n",
    "    numtrans = len(groceries)\n",
    "    rddT = sc.parallelize(groceries)\n",
    "    rddL1 = computeL1 ( rddT, numtrans, theta )\n",
    "    L1 = rddL1.keys().collect() # we need only the items (keys) in final L1\n",
    "    TL1 = rddT.mapPartitions( lambda seqOfT : computeTfilteredByL1( seqOfT, L1 )  )\n",
    "    rddC2T = TL1.mapPartitions( lambda seqOfFilteredT : generateC2( seqOfFilteredT ) )\n",
    "    rddC2TFlat = rddC2T.flatMap( lambda x : x )\n",
    "    rddL2 = computeL2( rddC2TFlat, numtrans, theta )\n",
    "    rddL2 = rddL2.sortBy(lambda a: -a[1])\n",
    "    return rddL2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f3c8a1-b8d2-41ed-936a-a081fb9c337a",
   "metadata": {},
   "source": [
    "## Exercice code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3e5504-e9de-4211-8124-c1cd3601f5aa",
   "metadata": {},
   "source": [
    "Read the dataset and convert it to transactions doing agrupations by member number and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7b26faa-f84e-46c1-ba3d-08675571c47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quimpm/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "  warnings.warn(\"Please install psutil to have better \" \"support with spilling\")\n",
      "/Users/quimpm/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "  warnings.warn(\"Please install psutil to have better \" \"support with spilling\")\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14963"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groceries_trans = sc.textFile(\"Groceries_dataset.csv\")\\\n",
    "              .map(lambda f : ((f.split(\",\")[0], f.split(\",\")[1]), f.split(\",\")[2]))\\\n",
    "              .groupByKey()\\\n",
    "              .map(lambda x : list(x[1]))\\\n",
    "              .collect()[1:]\n",
    "groceries_trans[:5]\n",
    "len(groceries_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55ba777-6814-4751-9549-9b805fd7f4c9",
   "metadata": {},
   "source": [
    "Calculate two iterations of Apriori algorithm for the new data and return L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b949e1-17d9-478e-8e40-e195d3fcec62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quimpm/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "  warnings.warn(\"Please install psutil to have better \" \"support with spilling\")\n",
      "/Users/quimpm/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "  warnings.warn(\"Please install psutil to have better \" \"support with spilling\")\n",
      "/Users/quimpm/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "  warnings.warn(\"Please install psutil to have better \" \"support with spilling\")\n",
      "/Users/quimpm/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "  warnings.warn(\"Please install psutil to have better \" \"support with spilling\")\n",
      "/Users/quimpm/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "  warnings.warn(\"Please install psutil to have better \" \"support with spilling\")\n",
      "/Users/quimpm/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "  warnings.warn(\"Please install psutil to have better \" \"support with spilling\")\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('other vegetables', 'whole milk'), 243),\n",
       " (('rolls/buns', 'whole milk'), 227),\n",
       " (('soda', 'whole milk'), 199),\n",
       " (('whole milk', 'yogurt'), 183),\n",
       " (('other vegetables', 'rolls/buns'), 182),\n",
       " (('other vegetables', 'soda'), 160)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_001 = calculate_two_iterations(groceries_trans, 0.01).collect()\n",
    "L2_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf5bc674-5eeb-455a-8592-f469b253be94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_01 = calculate_two_iterations(groceries_trans, 0.1).collect()\n",
    "L2_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7fb4dfe-5cf2-4ca4-9c0f-6766b55767cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_015 = calculate_two_iterations(groceries_trans, 0.15).collect()\n",
    "L2_015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba825e7-3d6d-4769-814b-68668763545d",
   "metadata": {},
   "source": [
    "The value of theta represents the frequency in which one item or groups of items appear into the total amnount of transactions. As can be seen, by setting a low theta value we are obtaining some pairs of items that are 0,01-frequent. But as we rise this value, we see how we don't find any further value. \n",
    "\n",
    "Let's create the assosiation rules and calculate the confidence and interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c37bcd8-359c-46eb-855f-44d8d4c0d846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('other vegetables', 'whole milk'),\n",
       " ('whole milk', 'other vegetables'),\n",
       " ('rolls/buns', 'whole milk'),\n",
       " ('whole milk', 'rolls/buns'),\n",
       " ('soda', 'whole milk'),\n",
       " ('whole milk', 'soda'),\n",
       " ('whole milk', 'yogurt'),\n",
       " ('yogurt', 'whole milk'),\n",
       " ('other vegetables', 'rolls/buns'),\n",
       " ('rolls/buns', 'other vegetables'),\n",
       " ('other vegetables', 'soda'),\n",
       " ('soda', 'other vegetables')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_first_element(pairs):\n",
    "    for pair in pairs:\n",
    "        yield pair[0]\n",
    "    \n",
    "rddT = sc.parallelize(L2_001)\n",
    "assosiation_rules = rddT.mapPartitions(lambda x: get_first_element(x)).map(lambda x: [(x[0],x[1]), (x[1],x[0])]).flatMap(lambda x: x)\n",
    "assosiation_rules.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c1668c-ecde-4107-863a-e982179312fe",
   "metadata": {},
   "source": [
    "Computing L1 for calculating the confidence and the interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "494cb03d-4916-49c9-b79c-d360117d85f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.01\n",
    "numtrans = len(groceries_trans)\n",
    "rddT = sc.parallelize(groceries_trans)\n",
    "rddL1 = computeL1 ( rddT, numtrans, theta )\n",
    "L1 = dict(rddL1.collect())\n",
    "L2 = dict(L2_001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1865d0ab-685b-441c-bb00-9f9f7ea02792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateConfidence(rule, L1, L2):\n",
    "    return L2[rule]/L1[rule[0]]\n",
    "\n",
    "def calculateInterest(rule, L1, confidence):\n",
    "    return confidence - (L1[rule[0]]/numtrans)\n",
    "\n",
    "def calculateMeasures(rules, L1, L2):\n",
    "    for rule in rules:\n",
    "        if rule in L2:\n",
    "            confidence = calculateConfidence(rule, L1, L2)\n",
    "            interest = calculateInterest(rule, L1, confidence)\n",
    "            yield(rule, confidence, interest)\n",
    "    \n",
    "assosiation_rules_measures = assosiation_rules.mapPartitions(lambda rules : calculateMeasures(rules, L1, L2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "630b06b3-219d-445b-ada0-795af41617fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('soda', 'whole milk'), 0.13143989431968295, 0.030256976455618256),\n",
       " (('rolls/buns', 'whole milk'), 0.13228438228438227, 0.017601497836076452),\n",
       " (('other vegetables', 'whole milk'),\n",
       "  0.1280295047418335,\n",
       "  0.0011832840641619047),\n",
       " (('other vegetables', 'rolls/buns'),\n",
       "  0.0958904109589041,\n",
       "  -0.03095580971876749),\n",
       " (('other vegetables', 'soda'), 0.08429926238145416, -0.042546958296217435),\n",
       " (('whole milk', 'yogurt'), 0.07314148681055156, -0.09407097058435589)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assosiation_rules_measures.sortBy(lambda x: x[2],False).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
